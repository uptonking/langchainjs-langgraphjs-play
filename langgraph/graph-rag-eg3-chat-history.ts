import { CheerioWebBaseLoader } from '@langchain/community/document_loaders/web/cheerio';
import { Document } from '@langchain/core/documents';
import {
  AIMessage,
  BaseMessage,
  HumanMessage,
  isAIMessage,
  SystemMessage,
  ToolMessage,
} from '@langchain/core/messages';
import { ChatPromptTemplate, PromptTemplate } from '@langchain/core/prompts';
import { tool } from '@langchain/core/tools';
import {
  Annotation,
  END,
  MemorySaver,
  MessagesAnnotation,
  START,
  StateGraph,
} from '@langchain/langgraph';
import { ToolNode, toolsCondition } from '@langchain/langgraph/prebuilt';
import { ChatOpenAI, OpenAIClient, OpenAIEmbeddings } from '@langchain/openai';
import { RecursiveCharacterTextSplitter } from '@langchain/textsplitters';
import { pull } from 'langchain/hub';
import { createRetrieverTool } from 'langchain/tools/retriever';
import { MemoryVectorStore } from 'langchain/vectorstores/memory';
import { z } from 'zod';

// üßë‚Äçüè´ [Build a RAG App: Part 2](https://js.langchain.com/docs/tutorials/qa_chat_history/)

const llm = new ChatOpenAI({
  // model: 'qwen/qwen3-4b-2507',
  model: 'google/gemma-3-12b',
  configuration: {
    baseURL: 'http://localhost:1234/v1',
    apiKey: 'not-needed',
  },
  temperature: 0,
});

// const embeddings = new OpenAIEmbeddings({
//   model: "text-embedding-qwen3-embedding-0.6b",
//   // model: 'text-embedding-embeddinggemma-300m',
//   configuration: {
//     baseURL: 'http://localhost:1234/v1',
//     // check: false,
//     apiKey: 'not-needed',
//   },
// });

const urls = [
  'https://dev.to/nyxtom/introduction-to-crdts-for-realtime-collaboration-2eb1',
  'https://dev.to/foxgem/crdts-achieving-eventual-consistency-in-distributed-systems-296g',
  // "https://lilianweng.github.io/posts/2023-06-23-agent/",
];

const docs = await Promise.all(
  urls.map((url) =>
    new CheerioWebBaseLoader(url, {
      selector: '.crayons-layout__content',
      // selector: 'p'
    }).load(),
  ),
);
const docsList = docs.flat();

// const cheerioLoader = new CheerioWebBaseLoader(
//   "https://lilianweng.github.io/posts/2023-06-23-agent/",
//   {
//     selector: 'p'
//   }
// );
// const docsList = await cheerioLoader.load();

console.log(';; docsList ', docsList[0].pageContent.length);
// console.log(';; docsList ', docsList[0].pageContent.slice(0, 2200))

const textSplitter = new RecursiveCharacterTextSplitter({
  chunkSize: 500,
  chunkOverlap: 50,
});
const docSplits = await textSplitter.splitDocuments(docsList);
console.log(';; docSplits ', docSplits.length);
// console.log(';; docSplits ', docSplits.slice(0, 6))

// üõ¢Ô∏è save embeddings to vectorDB
// const vectorStore = new MemoryVectorStore(embeddings);
// await vectorStore.addDocuments(docSplits)
// const vectorStore = await MemoryVectorStore.fromDocuments(
//   docSplits,
//   embeddings
// );
const openAiClient = new OpenAIClient({
  apiKey: 'not-needed',
  baseURL: 'http://localhost:1234/v1',
});

// Create a proper embeddings interface for OpenAIClient
class OpenAIClientEmbeddings {
  constructor(
    private client: OpenAIClient,
    private model: string,
  ) {}

  async embedDocuments(texts: string[]): Promise<number[][]> {
    const response = await this.client.embeddings.create({
      model: this.model,
      input: texts,
      encoding_format: 'float',
    });
    return response.data.map((item) => item.embedding);
  }

  async embedQuery(text: string): Promise<number[]> {
    const embeddings = await this.embedDocuments([text]);
    return embeddings[0];
  }
}

// Create embeddings instance and use fromDocuments
const embeddingsInstance = new OpenAIClientEmbeddings(
  openAiClient,
  'text-embedding-qwen3-embedding-0.6b',
);
// const embeddingsInstance = new OpenAIClientEmbeddings(openAiClient, 'text-embedding-embeddinggemma-300m');
// const embeddingsInstance = new OpenAIClientEmbeddings(openAiClient, 'text-embedding-granite-embedding-278m-multilingual');
const vectorStore = await MemoryVectorStore.fromDocuments(
  docSplits,
  embeddingsInstance,
);

const retrieveSchema = z.object({ query: z.string() });

// turning retrieval step into a tool has another benefit, which is that the query for the retrieval is generated by our model.
// query analysis allows a model to rewrite user queries into more effective search queries
const retrieve = tool(
  async ({ query }) => {
    const retrievedDocs = await vectorStore.similaritySearch(query, 2);
    const serialized = retrievedDocs
      .map(
        (doc) => `Source: ${doc.metadata.source}\nContent: ${doc.pageContent}`,
      )
      .join('\n');
    return [serialized, retrievedDocs];
  },
  {
    name: 'retrieve',
    description: 'Retrieve information related to a query.',
    schema: retrieveSchema,
    responseFormat: 'content_and_artifact',
  },
);

// Step 1: Generate an AIMessage that may include a tool-call to be sent.
async function queryOrRespond(state: typeof MessagesAnnotation.State) {
  const llmWithTools = llm.bindTools([retrieve]);
  const response = await llmWithTools.invoke(state.messages);
  // MessagesState appends messages to state instead of overwriting
  return { messages: [response] };
}

// Step 2: Execute the retrieval.
const tools = new ToolNode([retrieve]);

// Step 3: Generate a response using the retrieved content.
async function generate(state: typeof MessagesAnnotation.State) {
  // Get generated ToolMessages
  const recentToolMessages: ToolMessage[] = [];
  for (let i = state['messages'].length - 1; i >= 0; i--) {
    const message = state['messages'][i];
    if (message instanceof ToolMessage) {
      recentToolMessages.push(message);
    } else {
      break;
    }
  }
  const toolMessages = recentToolMessages.reverse();

  // Format into prompt
  const docsContent = toolMessages.map((doc) => doc.content).join('\n');
  const systemMessageContent =
    'You are an assistant for question-answering tasks. ' +
    'Use the following pieces of retrieved context to answer ' +
    "the question. If you don't know the answer, say that you " +
    "don't know. Use three sentences maximum and keep the " +
    'answer concise.' +
    '\n\n' +
    `${docsContent}`;

  const conversationMessages = state.messages.filter(
    (message) =>
      message instanceof HumanMessage ||
      message instanceof SystemMessage ||
      (message instanceof AIMessage && message.tool_calls?.length === 0),
  );
  const prompt = [
    new SystemMessage(systemMessageContent),
    ...conversationMessages,
  ];

  // Run
  const response = await llm.invoke(prompt);
  return { messages: [response] };
}

// Compile application and test
const graphBuilder = new StateGraph(MessagesAnnotation)
  .addNode('queryOrRespond', queryOrRespond)
  .addNode('tools', tools)
  .addNode('generate', generate)
  .addEdge('__start__', 'queryOrRespond')
  .addConditionalEdges('queryOrRespond', toolsCondition, {
    __end__: '__end__',
    tools: 'tools',
  })
  .addEdge('tools', 'generate')
  .addEdge('generate', '__end__');

const checkpointer = new MemorySaver();
const graph = graphBuilder.compile({ checkpointer });
// const graph = graphBuilder.compile();

// -------

// Specify an ID for the thread
const threadConfig = {
  configurable: { thread_id: 'abc123' },
  streamMode: 'values' as const,
};

const prettyPrint = (message: BaseMessage) => {
  let txt = `[${message._getType()}]: ${message.content}`;
  if ((isAIMessage(message) && message.tool_calls?.length) || 0 > 0) {
    const tool_calls = (message as AIMessage)?.tool_calls
      ?.map((tc) => `- ${tc.name}(${JSON.stringify(tc.args)})`)
      .join('\n');
    txt += ` \nTools: \n${tool_calls}`;
  }
  console.log(txt);
};

const inputs1 = { messages: [{ role: 'user', content: 'What is yjs ?' }] };

for await (const step of await graph.stream(inputs1, threadConfig)) {
  const lastMessage = step.messages[step.messages.length - 1];
  prettyPrint(lastMessage);
  console.log('-----\n');
}

const inputs2 = {
  messages: [
    {
      role: 'user',
      content:
        'are there any popular products or companies or github repos using it ? ',
    },
  ],
};

for await (const step of await graph.stream(inputs2, threadConfig)) {
  const lastMessage = step.messages[step.messages.length - 1];
  prettyPrint(lastMessage);
  console.log('-----\n');
}
